import sqlite3
import os
import shutil
import urllib.parse

# è¨­å®š
DB_PATH = "seo_content.db"
DOCS_DIR = "docs"
# è¨˜äº‹ã‚’æ ¼ç´ã™ã‚‹ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆæ•´ç†ç”¨ï¼‰
ARTICLES_DIR = os.path.join(DOCS_DIR, "articles")

def get_db_connection():
    return sqlite3.connect(DB_PATH)

def init_docs_structure():
    """ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ã®åˆæœŸåŒ–"""
    os.makedirs(ARTICLES_DIR, exist_ok=True)

def create_search_buttons_md(title):
    """è¨˜äº‹æœ«å°¾ã®æ¤œç´¢ãƒœã‚¿ãƒ³Markdownã‚’ä½œæˆ"""
    encoded_title = urllib.parse.quote(title)
    amazon_url = f"https://www.amazon.co.jp/s?k={encoded_title}"
    rakuten_url = f"https://search.rakuten.co.jp/search/mall/{encoded_title}"
    yahoo_url = f"https://shopping.yahoo.co.jp/search?p={encoded_title}"

    return f"""
## ğŸ›ï¸ ã“ã®å•†å“ã‚’ã•ãŒã™
<div class="grid cards" markdown>
-   [:material-cart: Amazonã§æ¢ã™]({amazon_url})
-   [:material-store: æ¥½å¤©å¸‚å ´ã§æ¢ã™]({rakuten_url})
-   [:material-shopping: Yahoo!ã§æ¢ã™]({yahoo_url})
</div>
"""

def update_index_page(articles):
    """ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸(index.md)ã«æ–°ç€è¨˜äº‹ãƒªã‚¹ãƒˆã‚’æ›¸ãè¾¼ã‚€"""
    index_path = os.path.join(DOCS_DIR, "index.md")
    
    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã®å›ºå®šãƒ˜ãƒƒãƒ€ãƒ¼éƒ¨åˆ†
    header = """# AI Tools & Gadget DB
ã‚ˆã†ã“ãã€‚ã“ã“ã¯AIã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸã‚¬ã‚¸ã‚§ãƒƒãƒˆãƒ»ãƒ„ãƒ¼ãƒ«æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚

## ğŸ†• æ–°ç€è¨˜äº‹ä¸€è¦§
"""
    
    with open(index_path, "w", encoding="utf-8") as f:
        f.write(header)
        
        # æ–°ã—ã„é †ã«ãƒªãƒ³ã‚¯ã‚’æ›¸ãè¾¼ã‚€
        # articles ã¯ (filename, title, category) ã®ãƒªã‚¹ãƒˆæƒ³å®š
        for filename, title, category in articles:
            # ãƒªãƒ³ã‚¯å…ˆã¯ articles/filename
            link = f"articles/{filename}"
            f.write(f"- [{title}]({link}) <small>({category})</small>\n")

def export_article_to_markdown():
    """DBã‹ã‚‰è¨˜äº‹ã‚’èª­ã¿å‡ºã—ã€MDãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ ï¼† index.mdæ›´æ–°"""
    init_docs_structure()
    conn = get_db_connection()
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()

    cursor.execute("SELECT * FROM products ORDER BY scraped_at DESC")
    rows = cursor.fetchall()

    exported_articles = []

    for row in rows:
        title = row["title"]
        body = row["generated_body"]
        category = row["category"]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’URLãƒãƒƒã‚·ãƒ¥ã‚„IDã‹ã‚‰æ±ºå®šï¼ˆãªã‘ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é©å½“ã«ï¼‰
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«urlã®ãƒãƒƒã‚·ãƒ¥å€¤ã®ä¸€éƒ¨ã‚’ä½¿ã†ã‹ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã«åˆã‚ã›ã‚‹
        # DBã«urlãŒã‚ã‚‹å‰æ
        url_hash = row["url"].split("/")[-1].replace(".html", "")
        if not url_hash:
             # ä¸‡ãŒä¸€ãƒãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
             import hashlib
             url_hash = hashlib.md5(row["url"].encode()).hexdigest()
             
        filename = f"{url_hash}.md"
        filepath = os.path.join(ARTICLES_DIR, filename)

        # æœ¬æ–‡ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not body:
            continue

        # æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’è¿½åŠ 
        search_buttons = create_search_buttons_md(title)
        
        full_content = f"# {title}\n\n{body}\n\n{search_buttons}"

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(full_content)
        
        print(f"Exported: {filename}")
        exported_articles.append((filename, title, category))

    # æœ€å¾Œã«ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°
    update_index_page(exported_articles)
    print("âœ… index.md has been updated with new articles.")

    conn.close()

def main():
    export_article_to_markdown()

if __name__ == "__main__":
    main()